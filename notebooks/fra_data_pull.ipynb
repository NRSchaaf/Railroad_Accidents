{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weathering the Rails: Federal Railroad Administration Accident/Incident Data API\n",
    "**Author:** Nathan Schaaf<br>\n",
    "**Date:** December 10, 2024<br>\n",
    "**Course:** Advanced Business Analytics, The Univerisity of North Carolina at Charlotte<br>\n",
    "**Professional Context:** Prepared for the U.S. Railroad Industry (with focus on safety improvements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\n",
    "<ol>\n",
    "<li>Prerequisites:</li>\n",
    "<ul>\n",
    "<li>Install the required Python libraries: pandas, requests, and datetime.</li>\n",
    "<li>Adjust the url variable to the endpoint URL for the FRA dataset.</li>\n",
    "</ul>\n",
    "<li>Adjust Parameters:</li>\n",
    "<ul>\n",
    "<li>Set the limit variable to define the number of rows per request (default is 1,000).</li>\n",
    "<li>The offset variable controls the starting point for each batch. The script automatically increments this value.</li>\n",
    "</ul>\n",
    "<li>Run the Script:</li>\n",
    "<ul>\n",
    "<li>Execute the script to fetch data in batches. Each batch will append data to a list until the API returns an empty response, signaling that all available records have been retrieved.</li>\n",
    "<li>Depending on the size of the dataset and your connection speed, the script may take several minutes to complete.</li>\n",
    "</ul>\n",
    "<li>Save the Data:</li>\n",
    "<ul>\n",
    "<li>The data is stored in a pandas DataFrame, which you can save to a CSV file or use directly in further analysis.</li>\n",
    "<li>Uncomment the print statements to display the total number of records retrieved and preview the first few rows.</li>\n",
    "</ul>\n",
    "<li>Caveats and Considerations:</li>\n",
    "<ul>\n",
    "<li>The API may occasionally fail or return incomplete data. If an error occurs, note the offset where the failure happened and restart the script from that point.</li>\n",
    "<li>This script is designed for large-scale data retrieval, so ensure sufficient system memory is available for handling large datasets.</li>\n",
    "</ol>\n",
    "<br>\n",
    "<br>\n",
    "<strong>Conclusion</strong>\n",
    "<p>By following these instructions, you can efficiently pull large amounts of FRA safety data for analysis or research purposes. This script offers flexibility to adapt to different dataset sizes while maintaining compliance with API limitations. <strong>Note</strong> this code will produce a dataset for the <strong>past 10 years</strong> from today.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING can take up to 6 minutes to download.\n",
    "# API URL\n",
    "url = \"https://data.transportation.gov/resource/85tf-25kj.json\"\n",
    "\n",
    "# Set the parameters\n",
    "limit = 1000  # The number of rows to fetch per request\n",
    "offset = 0    # The starting point for the next batch of rows\n",
    "all_data = [] # To store all the data\n",
    "\n",
    "while True:\n",
    "    # Create the query string with the limit and offset\n",
    "    query_url = f\"{url}?$limit={limit}&$offset={offset}\"\n",
    "    \n",
    "    # Make the API request\n",
    "    response = requests.get(query_url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Load the response into JSON format\n",
    "        data = response.json()\n",
    "        \n",
    "        # If no data is returned, we've reached the end\n",
    "        if not data:\n",
    "            break\n",
    "        \n",
    "        # Append the data to our list\n",
    "        all_data.extend(data)\n",
    "        \n",
    "        # Update the offset for the next batch of rows\n",
    "        offset += limit\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "# Convert the list of records into a pandas DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Display the number of rows fetched\n",
    "#print(f\"Total records retrieved: {len(df)}\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import primary accident cause codes related to track issues\n",
    "from primaryAccidentCodesLibrary import primary_accident_cause_codes\n",
    "\n",
    "# Get a list of track-related codes\n",
    "codes_list = list(primary_accident_cause_codes.keys())\n",
    "\n",
    "# Exclude rows with 'primaryaccidentcausecode' in the track-related codes list\n",
    "non_track_accidents_df = df[~df['primaryaccidentcausecode'].isin(codes_list)].copy()\n",
    "\n",
    "# Select relevant features for analysis\n",
    "features_to_analyze = ['reportingrailroadcode', 'accidentnumber', 'date', 'time', 'accidenttype', 'hazmatreleasedcars', 'station', 'stateabbr', 'temperature', 'visibility_code', 'visibility', 'weathercondition', 'tracktype', 'equipmenttype', 'trainspeed', 'equipmentdamagecost', 'trackdamagecost', 'totaldamagecost', 'primaryaccidentcausecode', 'latitude', 'longitude']\n",
    "\n",
    "# Drop all columns except the selected ones\n",
    "non_track_accidents_df = non_track_accidents_df[features_to_analyze]\n",
    "\n",
    "# Display the first few rows of the new DataFrame\n",
    "#non_track_accidents_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The filtered incidents occurred between 2014-11-16 and 2024-08-31.\n"
     ]
    }
   ],
   "source": [
    "# Ensure the 'date' column is in datetime format\n",
    "non_track_accidents_df['date'] = pd.to_datetime(non_track_accidents_df['date'])\n",
    "\n",
    "# Define the date 10 years ago from today\n",
    "ten_years_ago = datetime.today() - timedelta(days=365*10)\n",
    "\n",
    "# Filter the DataFrame to only include incidents from the past 10 years\n",
    "non_track_accidents_df = non_track_accidents_df[non_track_accidents_df['date'] >= ten_years_ago]\n",
    "\n",
    "# Check the new date range\n",
    "date_min = non_track_accidents_df['date'].min()\n",
    "date_max = non_track_accidents_df['date'].max()\n",
    "\n",
    "print(f\"The filtered incidents occurred between {date_min.date()} and {date_max.date()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the filtered DataFrame to a CSV file\n",
    "non_track_accidents_df.to_csv(\"non_track_accidents_last_10_years.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
