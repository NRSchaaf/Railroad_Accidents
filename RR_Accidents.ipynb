{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Railroad Track Failures Due to Extreme Temperature Flucuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Definition & Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective/Thesis\n",
    "<p>The objective of this project is to develop a machine learning model that helps to predict potential derailment locations specifically caused by track failures due to the result of extreme temperatures fluctuations. By integrating national weather forecasts with historical derailment and track data, the model will identify high-risk areas where derailments are most likely to occur based off the weather forecast. This analysis aims to enable predictive maintenance strategies, focusing on increasing track inspections and repairs during and immediately after periods of extreme weather conditions to mitigate derailment risks.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope\n",
    "<ul>\n",
    "    <li><strong>Data Collection</strong>: Gathering historical data on derailments, environmental factors (temperature fluctuations), track conditions, and weather forecasts.</li>\n",
    "    <li><strong>Model Development</strong>: Building and validating a machine learning model capable of predicting derailment locations based on extreme weather patterns. [Possible options: Logistic Regression, Decision Trees, Random Forest, Gradient Boosting, Neural Networks.]</li>\n",
    "    <li><strong>Geospatial Analysis</strong>: Applying geospatial methods to identify high-risk regions for derailments due to wide gauge.</li>\n",
    "    <li><strong>Real-Time Forecast Integration</strong>: Integrating national weather forecasts to provide real-time predictions of potential derailment locations.</li>\n",
    "    <li><strong>Actionable Insights</strong>: Recommending areas for increased track maintenance and inspection based on predictions, with a focus on preventing accidents caused by wide gauge in extreme weather conditions.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance\n",
    "<p>Track failures are the leading cause of non-reportable derailments, with over 12,000 reportable events recorded, according to the Federal Railroad Administration (FRA). As climate change contributes to increasingly unpredictable and extreme weather patterns, the risk of derailments due to wide gauge is likely to rise. This project is significant because it will provide a data-driven approach to mitigating these risks. By predicting where derailments are likely to occur, rail companies can proactively focus maintenance and inspection efforts, reducing the likelihood of accidents, protecting human lives, and preserving infrastructure.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Railroad Accident Data from Federal Railroad Administration (FRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING can take up to 6 minutes to download.\n",
    "# API URL\n",
    "url = \"https://data.transportation.gov/resource/85tf-25kj.json\"\n",
    "\n",
    "# Set the parameters\n",
    "limit = 1000  # The number of rows to fetch per request\n",
    "offset = 0    # The starting point for the next batch of rows\n",
    "all_data = [] # To store all the data\n",
    "\n",
    "while True:\n",
    "    # Create the query string with the limit and offset\n",
    "    query_url = f\"{url}?$limit={limit}&$offset={offset}\"\n",
    "    \n",
    "    # Make the API request\n",
    "    response = requests.get(query_url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Load the response into JSON format\n",
    "        data = response.json()\n",
    "        \n",
    "        # If no data is returned, we've reached the end\n",
    "        if not data:\n",
    "            break\n",
    "        \n",
    "        # Append the data to our list\n",
    "        all_data.extend(data)\n",
    "        \n",
    "        # Update the offset for the next batch of rows\n",
    "        offset += limit\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "# Convert the list of records into a pandas DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Display the number of rows fetched\n",
    "#print(f\"Total records retrieved: {len(df)}\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all features\n",
    "with pd.option_context('display.max_columns', None):  # Adjust pandas to temporarily display all features\n",
    "    features = list(df.columns)\n",
    "    print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Analysis Dataframe of RR incidents with a Track-Related Primary Accident Cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Track Accident DataFrame to filter only incidents with an identified \"primaryaccidentcause\" of a track type.\n",
    "from primaryAccidentCodesLibrary import primary_accident_cause_codes\n",
    "\n",
    "# Get a list of codes from the dictionary\n",
    "codes_list = list(primary_accident_cause_codes.keys())\n",
    "\n",
    "# Create Track Accident DataFrame to filter only incidents with an identified \"primaryaccidentcause\"\n",
    "track_accidents_df = df[df['primaryaccidentcausecode'].isin(codes_list)].copy()\n",
    "\n",
    "# Relevant features to include in the analysis based on domain knowledge.\n",
    "features_to_analyze = ['reportingrailroadcode', 'accidentnumber', 'date', 'time', 'accidenttype', 'hazmatreleasedcars', 'station', 'stateabbr', 'temperature', 'visibility_code', 'visibility', 'weathercondition', 'tracktype', 'equipmenttype', 'trainspeed', 'equipmentdamagecost', 'trackdamagecost', 'totaldamagecost', 'primaryaccidentcausecode', 'latitude', 'longitude']\n",
    "\n",
    "# Drop all columns except the selected ones\n",
    "track_accidents_df = track_accidents_df[features_to_analyze]\n",
    "\n",
    "# Display the first few rows of the new DataFrame\n",
    "#track_accidents_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the date range of incidents in our query.\n",
    "\n",
    "# Drop rows where 'accidenttype' or 'visibility' have missing values\n",
    "track_accidents_df.dropna(subset=['accidenttype', 'visibility'], inplace=True)\n",
    "\n",
    "# Ensure the 'date' column is in datetime format\n",
    "track_accidents_df['date'] = pd.to_datetime(track_accidents_df['date'])\n",
    "\n",
    "# Define the date 10 years ago from today\n",
    "ten_years_ago = datetime.today() - timedelta(days=365*10)\n",
    "\n",
    "# Filter the DataFrame to only include incidents from the past 10 years\n",
    "track_accidents_df = track_accidents_df[track_accidents_df['date'] >= ten_years_ago]\n",
    "\n",
    "# Check the new date range\n",
    "date_min = track_accidents_df['date'].min()\n",
    "date_max = track_accidents_df['date'].max()\n",
    "\n",
    "print(f\"The filtered incidents occurred between {date_min.date()} and {date_max.date()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_accidents_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = track_accidents_df.isnull().sum()\n",
    "null_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#track_accidents_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame to a CSV file\n",
    "#track_accidents_df.to_csv('track_accidents_last_10_years.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather Data from..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract GPS location and Date from the Track Accident Dataframe\n",
    "#track_accidents_df['incident_date'] = pd.to_datetime(track_accidents_df['incident_date']) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
